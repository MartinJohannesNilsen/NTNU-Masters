{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from csv import QUOTE_NONE\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import numpy as np\n",
    "sys.path.append(str(Path(os.path.abspath(\"\")).parents[0]))\n",
    "from utils.metrics import get_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(os.path.abspath(\"\")).parents[1]\n",
    "\n",
    "max_int = sys.maxsize\n",
    "while True:\n",
    "    # Decrease the value by factor 10 as long as the OverflowError occurs.\n",
    "    try:\n",
    "        csv.field_size_limit(max_int)\n",
    "        break\n",
    "    except OverflowError:\n",
    "        max_int = int(max_int/10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_path = Path(os.path.abspath(\"\")).parents[0] / \"pred_values\"\n",
    "\n",
    "model_vote_weighting = {\n",
    "    #\"svm\": 0.5111,\n",
    "    #\"gaussian_process\": 0.7359,\n",
    "    #\"albert\": 0.8527,\n",
    "    \"bert\": 0.8981,\n",
    "    \"distilbert\": 0.9077,\n",
    "    \"roberta\": 0.9276,\n",
    "    \"lstm\": 0.8816,\n",
    "    \"cnn\": 0.8569\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pred_paths = {\n",
    "    \"gaussian_process\": pred_path / \"classical_gp_bert_head_256_preds.csv\",\n",
    "    \"svm\": pred_path / \"classical_svm_liwc_2022_256_preprocessed_preds.csv\",\n",
    "    \"cnn\": pred_path / \"nn_cnn_preds.csv\",\n",
    "    \"lstm\": pred_path / \"nn_lstm_preds.csv\",\n",
    "    \"albert\": pred_path / \"llm_albert-base-v2_256_preds.csv\",\n",
    "    \"bert\": pred_path / \"llm_bert-base-uncased_256_preds.csv\",\n",
    "    \"distilbert\": pred_path / \"llm_distilbert-base-uncased_256_preds.csv\",\n",
    "    \"roberta\": pred_path / \"llm_roberta-base_256_preds.csv\"\n",
    "}\n",
    "\n",
    "def majority_vote_label_pred(models, weighted: bool = True, binary_input: bool = True):\n",
    "\n",
    "    \"\"\"\n",
    "    Makes all models deemed suited for classification cast a vote for what class each text belongs to\n",
    "    Returns\n",
    "    final_preds: list of final predictions (argmax of class 0 and 1 for each text)\n",
    "    true_labels: Gold standard labels for comparison\n",
    "    \"\"\"\n",
    "\n",
    "    all_model_preds = {model: pd.read_csv(model_pred_paths[model]) for model in models}\n",
    "    test_length = len(all_model_preds[\"roberta\"].index)\n",
    "    true_labels = all_model_preds[\"roberta\"][\"label\"].values\n",
    "\n",
    "    combined_preds = np.zeros((test_length, 2))\n",
    "    for model, preds in all_model_preds.items():\n",
    "        weight = model_vote_weighting[model] if weighted else 1\n",
    "\n",
    "        if binary_input:\n",
    "            preds = preds[\"pred_label\"].values\n",
    "            # Sum up all preds for both classes and weight the preds based on models' f2 scores\n",
    "            for i in range(test_length):\n",
    "                combined_preds[i][preds[i]] += weight**2\n",
    "        \n",
    "        else:\n",
    "            preds = preds[\"pred_val\"].values\n",
    "            for i in range(test_length):\n",
    "                pred_val = preds[i]\n",
    "                classification = 1 if pred_val > 0.5 else 0\n",
    "                combined_preds[i][classification] += pred_val*weight**2 # **2 should punish weaker models more\n",
    "\n",
    "    # Most voted for class wins classification\n",
    "    final_preds = [np.argmax(class_scores) for class_scores in combined_preds]\n",
    "\n",
    "    return final_preds, true_labels, combined_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = list(model_vote_weighting.keys())\n",
    "\n",
    "voted_labels, true_labels, combined_preds = majority_vote_label_pred(models, weighted=False, binary_input=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tn': 2513, 'fp': 19, 'fn': 17, 'tp': 426, 'accuracy': 0.9878991596638655, 'precision': 0.9573033707865168, 'recall': 0.9616252821670429, 'specificity': 0.9924960505529226, 'f1_score': 0.9594594594594594, 'f05_score': 0.9581646423751686, 'f2_score': 0.9607577807848443, 'roc_auc': 0.9770606663599827}\n"
     ]
    }
   ],
   "source": [
    "metrics = get_metrics(voted_labels, true_labels)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined_score_for_preds(combined_preds):\n",
    "    total_scores = []\n",
    "\n",
    "    for combined_pred in combined_preds:\n",
    "        total_scores.append(combined_pred[1]-combined_pred[0])\n",
    "\n",
    "    return total_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_misclassified(pred_labels, true_labels):\n",
    "    test_texts = pd.read_csv(Path(os.path.abspath(\"\")).parents[1] / \"dataset_creation/data/train_test/new/test_sliced_stair_twitter_256.csv\", sep=\"â€Ž\", quoting=QUOTE_NONE)\n",
    "\n",
    "    indexes = []\n",
    "    misclassified = []\n",
    "    preds = []\n",
    "    for idx, (pred, true) in enumerate(zip(pred_labels, true_labels)):\n",
    "        if int(pred) != int(true):\n",
    "            indexes.append(idx)\n",
    "            misclassified.append(str(test_texts[\"text\"].iloc[idx]))\n",
    "            preds.append((pred, true))\n",
    "\n",
    "    return indexes, misclassified, preds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 9 pred 0 score -5.0 true 1\n",
      "Yesssss!!!\n",
      "idx 23 pred 0 score -3.0 true 1\n",
      "I'm surprised most of America can dress their self.\n",
      "idx 442 pred 0 score -3.0 true 1\n",
      ":(\n",
      "idx 91 pred 0 score -3.0 true 1\n",
      "I'm getting over much of the problems I've had in the past, dacruz, the only reason I'm so worried about this whole thing is it seems to be rooted in truth. If a White Owl is a bad Omen, then I'd rather be prepared than unprepared.\n",
      "idx 366 pred 0 score -3.0 true 1\n",
      "Stickam Party!\n",
      "idx 422 pred 0 score -1.0 true 1\n",
      "Oh, dear... Is it wrong that I want to go to a hoedown now?\n",
      "idx 408 pred 0 score -1.0 true 1\n",
      "Tick tock NE...\n",
      "idx 308 pred 0 score -1.0 true 1\n",
      "Look up at the stars and you'll see the ones who have gone before you. A smile is always on their face. Why am I not in the stars?\n",
      "idx 255 pred 0 score -1.0 true 1\n",
      "I don't struggle with self worth. I struggle with patience. I have virtually zero patience for anyone.\n",
      "idx 345 pred 0 score -1.0 true 1\n",
      "matter, but it is simply not my fault. The University of Iowa authorities should be blamed for the unfortunate outcome. If the university had taken positive steps as it is supposed by the tax-payers, tuition payers and funding agencies, all this could be avoided. The University of Iowa is trying its best to cover . . . Nicholson in the DCS dissertation award, in spite of the fact that I am putting my whole career on the line. I am being a physicist who believed in the conservation of matter, energy, momentum, etc. Although my flesh/blood-made body seems dead, my spiritual soul remains perpetual and I am being quantum leaping to another corner of our world. I have finished what I am sup- posed to do here which is to make right what was once wrong. I am proud of my achievement here and I am more confident in my upcoming journey. So long my friends, maybe we will meet again in another time at another place. May the lord bless all those descent human beings who are honest, hard working and truthy.\n",
      "idx 243 pred 0 score -1.0 true 1\n",
      "Your post confirms you've never wrestled, MnM.\n",
      "idx 199 pred 0 score -1.0 true 1\n",
      "That water better had not be flavored!\n",
      "idx 196 pred 0 score -1.0 true 1\n",
      "Wtf are you doing, NE??!\n",
      "idx 186 pred 0 score -1.0 true 1\n",
      "I can do this...\n",
      "idx 182 pred 0 score -1.0 true 1\n",
      "they are under control, but me, I see all.\n",
      "idx 49 pred 0 score -1.0 true 1\n",
      "FACT - Girls &gt; Guys\n",
      "idx 252 pred 0 score -1.0 true 1\n",
      "Americans waking up like... #Election2016 #PresidentTrump #NotMyPresident #Vote2016 https://t.co/PnRo5qIPIu\n",
      "idx 1510 pred 1 score 1.0 true 0\n",
      "When I was younger I would record my favorite songs onto a piece of papyrus.\n",
      "idx 2702 pred 1 score 1.0 true 0\n",
      "What do you call a gun with three barrels?\n",
      "idx 2543 pred 1 score 1.0 true 0\n",
      "Those who died are justified, for wearing the badge, they're the chosen whites. You justify those that died by wearing the badge, they're the chosen whites\n",
      "idx 2143 pred 1 score 1.0 true 0\n",
      "You could die and nobody would care.\n",
      "idx 1949 pred 1 score 1.0 true 0\n",
      "To feel your pain, you feel mine. Go inside eachother's minds, just to see what we'd find. Look at shit through eachother's eyes.\n",
      "idx 1734 pred 1 score 1.0 true 0\n",
      "Hahaha the fact that you don't know how your being ignorant is ignorant.ðŸ˜­ðŸ˜‚\n",
      "idx 1512 pred 1 score 1.0 true 0\n",
      ".....ooh kill em oooh kill em.\n",
      "idx 1451 pred 1 score 1.0 true 0\n",
      "I was totally freaked out tonight while driving home with Janice at 12am. The black coupe next to me matched my speed even though I was going fast and then he continued to match me speed as I slowed down. So I turn to look at the driver of the car and he's wearing a mask. A rubber mask reminiscent to freddy cruger. 0_0 I almost had a heart attack. I then slowed down a lot and he continued on... oh my goodness...\n",
      "idx 476 pred 1 score 1.0 true 0\n",
      "I always want the blog to be the magical valve. Turn it on, write, be healed. It never is though. And now I still hurt, I just feel more raw now. As long as I stay with the emotion and don't deny...I'll be alright.\n",
      "idx 1283 pred 1 score 1.0 true 0\n",
      "Oh, look what you've done; you've made a fool of everyone.\n",
      "idx 1212 pred 1 score 1.0 true 0\n",
      "I'm a slave to all these voices in my head, And I'm afraid, I'm afraid of what they've said. I'm a slave. ðŸ‘Œ\n",
      "idx 1211 pred 1 score 1.0 true 0\n",
      "Mom woke me up today at noon; I was a little peeved. Stayed in bed for about ten minutes and went: Oh shit, it's Christmas isn't it. Completely forgot.\n",
      "idx 1032 pred 1 score 1.0 true 0\n",
      "We are not who you think we are. We are Golden.\n",
      "idx 1012 pred 1 score 1.0 true 0\n",
      "Don't worry about the people in your past, there's a reason they didn't make it to your future.\n",
      "idx 771 pred 1 score 1.0 true 0\n",
      "I love when people misinterpret brutally depressing songs. Like Born in the USA, or Happy Birthday.\n",
      "idx 678 pred 1 score 1.0 true 0\n",
      "You should watch Oblivion the ending is way better than Mass Effect 3.\n",
      "idx 2706 pred 1 score 1.0 true 0\n",
      "Mass Effect 2: It's nice to see that the Nigerian 419 scam is still going strong in 2184, this time with Batarians trying to recover quarantined Prothean artifacts.\n",
      "idx 1338 pred 1 score 1.0 true 0\n",
      "Idk what I did\n",
      "idx 2970 pred 1 score 1.0 true 0\n",
      "Some people are like Slinkies. They don't really have a purpose, but they still bring a smile to your face when you push them down the stairs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jonas\\AppData\\Local\\Temp\\ipykernel_17040\\426673700.py:2: ParserWarning: Falling back to the 'python' engine because the separator encoded in utf-8 is > 1 char long, and the 'c' engine does not support such separators; you can avoid this warning by specifying engine='python'.\n",
      "  test_texts = pd.read_csv(Path(os.path.abspath(\"\")).parents[1] / \"dataset_creation/data/train_test/new/test_sliced_stair_twitter_256.csv\", sep=\"â€Ž\", quoting=QUOTE_NONE)\n"
     ]
    }
   ],
   "source": [
    "indexes, misclassified, preds = get_misclassified(voted_labels, true_labels)\n",
    "\n",
    "total_scores = get_combined_score_for_preds([combined_preds[idx] for idx in indexes])\n",
    "total_scores_sorted_idx = np.argsort(total_scores)\n",
    "\n",
    "for idx in total_scores_sorted_idx:\n",
    "    print(f\"idx {indexes[idx]} pred {preds[idx][0]} score {total_scores[idx]} true {preds[idx][1]}\\n{misclassified[idx]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
