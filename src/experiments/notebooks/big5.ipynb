{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Data\n",
    "import pandas as pd\n",
    "\n",
    "# Embeddings and ML\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification\n",
    "from word_embeddings import get_bert_word_embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Dorian Gray with Rainbow Scarf #LoveWins (from...</td>\n",
       "      <td>smile annotations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>@SelectShowcase @Tate_StIves ... Replace with ...</td>\n",
       "      <td>smile annotations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>@Sofabsports thank you for following me back. ...</td>\n",
       "      <td>smile annotations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>@britishmuseum @TudorHistory What a beautiful ...</td>\n",
       "      <td>smile annotations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>@NationalGallery @ThePoldarkian I have always ...</td>\n",
       "      <td>smile annotations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5052</th>\n",
       "      <td>2016-09-10</td>\n",
       "      <td>\"\"\"You bet Ben was belting louder than any gir...</td>\n",
       "      <td>umass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5053</th>\n",
       "      <td>2016-09-10</td>\n",
       "      <td>One of my hobby @ Ma Hood https://t.co/SHJDDWQ8QB</td>\n",
       "      <td>umass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5054</th>\n",
       "      <td>2016-09-10</td>\n",
       "      <td>Another Cardigan Records Hopscotch Day Party i...</td>\n",
       "      <td>umass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5055</th>\n",
       "      <td>2016-09-11</td>\n",
       "      <td>Bachelorette üíçüíû @ Laurita Winery https://t.co/...</td>\n",
       "      <td>umass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5056</th>\n",
       "      <td>2016-09-11</td>\n",
       "      <td>\"\"\"When life gives you Lemon üçã\\nREAD ü§ìü§ìü§ìüëåüëåüëå‚òùÔ∏è‚òù...</td>\n",
       "      <td>umass</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5057 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date                                               text  \\\n",
       "0            NaN  Dorian Gray with Rainbow Scarf #LoveWins (from...   \n",
       "1            NaN  @SelectShowcase @Tate_StIves ... Replace with ...   \n",
       "2            NaN  @Sofabsports thank you for following me back. ...   \n",
       "3            NaN  @britishmuseum @TudorHistory What a beautiful ...   \n",
       "4            NaN  @NationalGallery @ThePoldarkian I have always ...   \n",
       "...          ...                                                ...   \n",
       "5052  2016-09-10  \"\"\"You bet Ben was belting louder than any gir...   \n",
       "5053  2016-09-10  One of my hobby @ Ma Hood https://t.co/SHJDDWQ8QB   \n",
       "5054  2016-09-10  Another Cardigan Records Hopscotch Day Party i...   \n",
       "5055  2016-09-11  Bachelorette üíçüíû @ Laurita Winery https://t.co/...   \n",
       "5056  2016-09-11  \"\"\"When life gives you Lemon üçã\\nREAD ü§ìü§ìü§ìüëåüëåüëå‚òùÔ∏è‚òù...   \n",
       "\n",
       "                   name  \n",
       "0     smile annotations  \n",
       "1     smile annotations  \n",
       "2     smile annotations  \n",
       "3     smile annotations  \n",
       "4     smile annotations  \n",
       "...                 ...  \n",
       "5052              umass  \n",
       "5053              umass  \n",
       "5054              umass  \n",
       "5055              umass  \n",
       "5056              umass  \n",
       "\n",
       "[5057 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from csv import QUOTE_NONE\n",
    "import sys\n",
    "import csv\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "base_path = Path(os.path.abspath(\"\")).parents[1] / \"dataset_creation\" / \"data\"\n",
    "datasets = {\n",
    "    \"school_shooters\": base_path / \"school_shooters.csv\",\n",
    "    \"manifestos\": base_path / \"manifestos.csv\",\n",
    "    \"stair_twitter_archive\": base_path / \"stair_twitter_archive.csv\",\n",
    "    \"twitter\": base_path / \"twitter.csv\",\n",
    "}\n",
    "\n",
    "schoolshootersinfo_df = pd.read_csv(datasets[\"school_shooters\"], encoding=\"utf-8\", delimiter=\"‚Äé\", engine=\"python\", quoting=QUOTE_NONE)\n",
    "manifesto_df = pd.read_csv(datasets[\"manifestos\"], encoding=\"utf-8\", delimiter=\"‚Äé\", engine=\"python\", quoting=QUOTE_NONE)\n",
    "stair_twitter_archive_df = pd.read_csv(datasets[\"stair_twitter_archive\"], encoding=\"utf-8\", delimiter=\"‚Äé\", engine=\"python\", quoting=QUOTE_NONE)\n",
    "twitter_df = pd.read_csv(datasets[\"twitter\"], encoding=\"utf-8\", delimiter=\"‚Äé\", engine=\"python\", quoting=QUOTE_NONE)\n",
    "twitter_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One may argue that it could be beneficial to start with a overweigth of regular users as the school shooters posts will by its nature be a fraction. Therefore, I start with the school shooters info dataset (~1000 rows) against twitter (~5000 rows)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embeddings intro\n",
    "\n",
    "We need to represent text as matrices/vectors for machine learning pipelines to understand and find/learn patterns. This is where embeddings come into play as we are taking on the domain of text. For word embeddings, we want to encapsulate the similarity between words. As a rule of thumb, words that are similar should appear together in the grid/3d space. The same applies for sentence and document embeddings. Initially, we look further into word embeddings specifically.\n",
    "\n",
    "The first approach is to use **word2vec**. This takes the cosine similarity but is not contextualized - this means that the word \"bank\" will have the same vectorial representation despite either meaning the place you havce your money or a long mound or slope, like a riverbank. **GloVe** is also context independent. The other alternatives, either **fasttext**, **elmo** or **BERT** is context dependent. \n",
    "\n",
    "Initially, we would like to use BERT embeddings as we want to utilize newer language models. Take a look at this [walkthrough](https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text preparation\n",
    "We create a list of all texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "schoolshooters_texts = schoolshootersinfo_df[\"text\"].to_list()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A note about token length\n",
    "\n",
    "We would need to ensure that each run of BERT does not exceed 512 tokens. Chunking is the approach to take, and one could then choose one of multiple strategies. Chunking per post and taking the average seems like the way to go. This way, we keep each post to themselves, as we later on want to detect on a post basis, but could also chunk all texts of an author and then be able to run all texts of an author through BERT (then all text would be seen as a post, only a definition thing). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"./model/pretrained_big_five\", num_labels=5)\n",
    "\n",
    "model.config.label2id = {\n",
    "    \"Extroversion\": 0,\n",
    "    \"Neuroticism\": 1,\n",
    "    \"Agreeableness\": 2,\n",
    "    \"Conscientiousness\": 3,\n",
    "    \"Openness\": 4,\n",
    "}\n",
    "model.config.id2label = {y: x for x, y in model.config.label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def pred(model_input: str) -> Dict[str, float]:\n",
    "    if len(model_input)<20:\n",
    "        ret = {\n",
    "            \"Extroversion\": float(0),\n",
    "            \"Neuroticism\": float(0),\n",
    "            \"Agreeableness\": float(0),\n",
    "            \"Conscientiousness\": float(0),\n",
    "            \"Openness\": float(0),\n",
    "        }\n",
    "        return ret\n",
    "    else:\n",
    "        dict = get_bert_word_embeddings(model_input, pretrained_tokenizer=\"./model/pretrained_big_five\", do_lower_case=True)\n",
    "        outs = model(**dict)\n",
    "        b_logit_pred = outs[0]\n",
    "        pred_label = torch.sigmoid(b_logit_pred)\n",
    "        ret = {\n",
    "            \"Extroversion\": float(pred_label[0][0]),\n",
    "            \"Neuroticism\": float(pred_label[0][1]),\n",
    "            \"Agreeableness\": float(pred_label[0][2]),\n",
    "            \"Conscientiousness\": float(pred_label[0][3]),\n",
    "            \"Openness\": float(pred_label[0][4])\n",
    "        }\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1343 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (582 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1322 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1597 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1184 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (946 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1043 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3074 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3071 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4735 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (945 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (562 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (684 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (744 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (602 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (978 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (534 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1048 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (813 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (752 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (638 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (679 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (554 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (946 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1453 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1176 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (825 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1150 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1176 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1040 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (681 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (613 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (787 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1144 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (742 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3013 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (997 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (586 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (819 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4000 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (664 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (607 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1276 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (534 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (727 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (703 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (753 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (561 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1433 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (752 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (663 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1265 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (609 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (890 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (686 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (903 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (616 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1056 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (834 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (567 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (765 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (685 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (625 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (645 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2063 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1547 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "preds = [pred(txt) for txt in schoolshooters_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Extroversion': 0.4780445992946625,\n",
       " 'Neuroticism': 0.5271245837211609,\n",
       " 'Agreeableness': 0.5256186127662659,\n",
       " 'Conscientiousness': 0.47671961784362793,\n",
       " 'Openness': 0.5333241820335388}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
